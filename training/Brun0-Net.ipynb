{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92b7d97",
   "metadata": {},
   "source": [
    "## Scaricamenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2f8d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "# download Glove Dataset\n",
    "if not path.exists(\"./dataset/glove/\"):\n",
    "    !wget https://huggingface.co/stanfordnlp/glove/resolve/main/glove.42B.300d.zip -O ./glove.6B.zip \n",
    "    !mkdir dataset/glove\n",
    "    !mv ./glove.6B.zip dataset/glove\n",
    "    !cd dataset/glove && unzip ./glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb80f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/kaggle-data-sets/1335671/2233309/upload/bert-base-cased.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220615%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220615T154743Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=518334c07352fa9a8e94ff0c790e2c776ae796c9298f47879e655c457bad451165430e377234ea3bf8a4761d6c96913ff909ac36b9a717920c7d586423a2f390915b6514436ec8728d597ba023d7535b6a7aacd3496bc79a9ed1ccdf9773a9ef98fbdddf1000feb14ec1a5694e5cdca7edc9408a5ab18a6283a25ef50aedfbfa86966de897e9a52ed1bf0ce4fe28550f356d08d821831e3c61759814dd3ba0c86dc1c00974d9ff00ba59a9b1ae1d4de5167ff8dc5ba81b3c3dc369fca93c5e94bd1ab8f22e12b070118fc9bb2b4f02a4e4311a17cf6a18a572932f9145b81d1302feb9cd59c6a0594ae3bf275b1edd480705a6a3300fc5d194f1e484f5fd9dc5\n",
    "\n",
    "!mv ./bert-base-cased.zip ./dataset\n",
    "!cd dataset && unzip bert-base-cased.zip && mv bert-base-cased bert2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb537e",
   "metadata": {},
   "source": [
    "## PreProcessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ffd110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 16:13:40.317679: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-15 16:13:40.317692: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-15 16:13:40.317702: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (80d80221f392): /proc/driver/nvidia/version does not exist\n",
      "2022-06-15 16:13:40.317903: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from os import walk\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from transformers import TFBertModel\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb7b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d96bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_indices(words):\n",
    "    enumeration = enumerate(words.unique()) # get unique words\n",
    "    \n",
    "    return list(enumeration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb7edc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def clean_text(text, huge_changes=False):\\n    # TODO: vedere se vanno rimosse oppure no !\\n    if huge_changes:\\n        ## Remove puncuation\\n        text = text.translate(string.punctuation)\\n\\n        ## Remove stop words\\n        text = text.lower().split() #default separator is any whitespace.\\n        stops = set(stopwords.words(\"english\"))\\n        text = [w for w in text if not w in stops and len(w) >= 3]   # no stop words and no words less than 3 chars\\n        text = \" \".join(text)    ## Clean the text\\n    \\n    ## Convert words to lower case and split them\\n    text = text.lower() # lo split solo per togliere le stopword\\n\\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\\/\\'+-=]\", \" \", text) #Here is a regex to match a string of characters that are not a letters or numbers 0-9 to match numbers\\n    text = re.sub(r\"what\\'s\", \"what is \", text)\\n    text = re.sub(r\"\\'s\", \" \", text)\\n    text = re.sub(r\"\\'ve\", \" have \", text)\\n    text = re.sub(r\"n\\'t\", \" not \", text)\\n    text = re.sub(r\"i\\'m\", \"i am \", text)\\n    text = re.sub(r\"\\'re\", \" are \", text)\\n    text = re.sub(r\"\\'d\", \" would \", text)\\n    text = re.sub(r\"\\'ll\", \" will \", text)\\n\\n    text = re.sub(r\"\\\\/\", \" \", text)\\n    #text = re.sub(r\"\\\\^\", \" ^ \", text)\\n    #text = re.sub(r\"\\\\+\", \" + \", text)\\n    #text = re.sub(r\"\\\\-\", \" - \", text)\\n    #text = re.sub(r\"\\\\=\", \" = \", text)\\n    text = re.sub(r\"\\'\", \" \", text)\\n    \\n    text = re.sub(r\"(\\\\d+)(k)\", r\"\\\\g<1>000\", text)\\n    text = re.sub(r\" e g \", \" eg \", text)\\n    text = re.sub(r\" b g \", \" bg \", text)\\n    text = re.sub(r\" u s \", \" american \", text)\\n    text = re.sub(r\"\\x00s\", \"0\", text)\\n    text = re.sub(r\" 9 11 \", \"911\", text)\\n    text = re.sub(r\"e - mail\", \"email\", text)\\n    text = re.sub(r\"j k\", \"jk\", text)\\n    text = re.sub(r\"\\\\s{2,}\", \" \", text)\\n    text = re.sub(r\"\\n\\n\", \" \", text)  \\n    \\'\\'\\'\\n    text = re.sub(r\"\\\\S,\\\\s\", \" ,\", text)  \\n    text = re.sub(r\"\\\\S,\\\\S\", \" , \", text)  \\n    text = re.sub(r\"\\\\S!\\\\s\", \" !\", text)\\n    text = re.sub(r\"\\\\S!\\\\S\", \" ! \", text)\\n    text = re.sub(r\"\\\\S\\\\?\\\\s\", \" ?\", text)\\n    text = re.sub(r\"\\\\S\\\\?\\\\S\", \" ? \", text)\\n    text = re.sub(r\"\\\\S\\\\.\\\\s\", \" .\", text)  \\n    text = re.sub(r\"\\\\S\\\\.\\\\S\", \" . \", text)\\n    text = re.sub(r\"\\\\S:\\\\s\", \" :\", text)\\n    text = re.sub(r\"\\\\S:\\\\S\", \" : \", text)\\n    \\'\\'\\'\\n    text = re.sub(\\'([\\\\.,!\\\\?():;\\\\-+=^])\\', r\\' \\x01 \\', text)\\n    text = re.sub(\\'\\\\s{2,}\\', \\' \\', text)\\n\\n    text = re.sub(r\"[0-9]+\", \" \", text)  \\n\\n    if huge_changes:\\n        text = re.sub(r\",\", \" \", text)\\n        text = re.sub(r\"\\\\.\", \" \", text)\\n        ## Stemming\\n        # TODO: vedere se sta roba puo servire\\n        text = text.split()\\n        #print(\"text2: \", text)\\n        stemmer = SnowballStemmer(\\'english\\')\\n        stemmed_words = [stemmer.stem(word) for word in text] \\n        text = \" \".join(stemmed_words)\\n\\n    return text '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def clean_text(text, huge_changes=False):\n",
    "    # TODO: vedere se vanno rimosse oppure no !\n",
    "    if huge_changes:\n",
    "        ## Remove puncuation\n",
    "        text = text.translate(string.punctuation)\n",
    "\n",
    "        ## Remove stop words\n",
    "        text = text.lower().split() #default separator is any whitespace.\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops and len(w) >= 3]   # no stop words and no words less than 3 chars\n",
    "        text = \" \".join(text)    ## Clean the text\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower() # lo split solo per togliere le stopword\n",
    "\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text) #Here is a regex to match a string of characters that are not a letters or numbers 0-9 to match numbers\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    #text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    #text = re.sub(r\"\\+\", \" + \", text)\n",
    "    #text = re.sub(r\"\\-\", \" - \", text)\n",
    "    #text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    \n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\n\\n\", \" \", text)  \n",
    "    '''\n",
    "    text = re.sub(r\"\\S,\\s\", \" ,\", text)  \n",
    "    text = re.sub(r\"\\S,\\S\", \" , \", text)  \n",
    "    text = re.sub(r\"\\S!\\s\", \" !\", text)\n",
    "    text = re.sub(r\"\\S!\\S\", \" ! \", text)\n",
    "    text = re.sub(r\"\\S\\?\\s\", \" ?\", text)\n",
    "    text = re.sub(r\"\\S\\?\\S\", \" ? \", text)\n",
    "    text = re.sub(r\"\\S\\.\\s\", \" .\", text)  \n",
    "    text = re.sub(r\"\\S\\.\\S\", \" . \", text)\n",
    "    text = re.sub(r\"\\S:\\s\", \" :\", text)\n",
    "    text = re.sub(r\"\\S:\\S\", \" : \", text)\n",
    "    '''\n",
    "    text = re.sub('([\\.,!\\?():;\\-+=^])', r' \\1 ', text)\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "\n",
    "    text = re.sub(r\"[0-9]+\", \" \", text)  \n",
    "\n",
    "    if huge_changes:\n",
    "        text = re.sub(r\",\", \" \", text)\n",
    "        text = re.sub(r\"\\.\", \" \", text)\n",
    "        ## Stemming\n",
    "        # TODO: vedere se sta roba puo servire\n",
    "        text = text.split()\n",
    "        #print(\"text2: \", text)\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text] \n",
    "        text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88cbb7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9704a709b505</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>On my perspective, I think that the face is a ...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c22adee811b6</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>I think that the face is a natural landform be...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a10d361e54e4</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>If life was on Mars, we would know by now. The...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db3e453ec4e2</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>People thought that the face was formed by ali...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id      essay_id  \\\n",
       "0  0013cc385424  007ACE74B050   \n",
       "1  9704a709b505  007ACE74B050   \n",
       "2  c22adee811b6  007ACE74B050   \n",
       "3  a10d361e54e4  007ACE74B050   \n",
       "4  db3e453ec4e2  007ACE74B050   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Hi, i'm Isaac, i'm going to be writing about h...           Lead   \n",
       "1  On my perspective, I think that the face is a ...       Position   \n",
       "2  I think that the face is a natural landform be...          Claim   \n",
       "3  If life was on Mars, we would know by now. The...       Evidence   \n",
       "4  People thought that the face was formed by ali...   Counterclaim   \n",
       "\n",
       "  discourse_effectiveness  \n",
       "0                Adequate  \n",
       "1                Adequate  \n",
       "2                Adequate  \n",
       "3                Adequate  \n",
       "4                Adequate  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(f'{dataset_path}/train.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72efdf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for new_value, old_value in type_label_index:\\n    dataset[\"type_label\"].replace(old_value, new_value, inplace=True) '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assegna degli indici alle label della colonna discourse_type e discourse_effectiveness\n",
    "#type_label_index = words_to_indices(dataset[\"discourse_type\"])\n",
    "effectiveness_label_index = words_to_indices(dataset[\"discourse_effectiveness\"])\n",
    "\n",
    "# creo le due nuove colonne\n",
    "#dataset[\"type_label\"] = dataset[\"discourse_type\"]\n",
    "dataset[\"effectiveness_label\"] = dataset['discourse_effectiveness']\n",
    "\n",
    "# rimpiazzo gli elementi dalle nuove colonne con quelli corretti (gli indici numerici)\n",
    "for new_value, old_value in effectiveness_label_index:\n",
    "    dataset[\"effectiveness_label\"].replace(old_value, new_value, inplace=True)\n",
    "\n",
    "\"\"\" for new_value, old_value in type_label_index:\n",
    "    dataset[\"type_label\"].replace(old_value, new_value, inplace=True) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0214235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>effectiveness_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9704a709b505</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>On my perspective, I think that the face is a ...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c22adee811b6</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>I think that the face is a natural landform be...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a10d361e54e4</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>If life was on Mars, we would know by now. The...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db3e453ec4e2</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>People thought that the face was formed by ali...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id      essay_id  \\\n",
       "0  0013cc385424  007ACE74B050   \n",
       "1  9704a709b505  007ACE74B050   \n",
       "2  c22adee811b6  007ACE74B050   \n",
       "3  a10d361e54e4  007ACE74B050   \n",
       "4  db3e453ec4e2  007ACE74B050   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Hi, i'm Isaac, i'm going to be writing about h...           Lead   \n",
       "1  On my perspective, I think that the face is a ...       Position   \n",
       "2  I think that the face is a natural landform be...          Claim   \n",
       "3  If life was on Mars, we would know by now. The...       Evidence   \n",
       "4  People thought that the face was formed by ali...   Counterclaim   \n",
       "\n",
       "  discourse_effectiveness  effectiveness_label  \n",
       "0                Adequate                    0  \n",
       "1                Adequate                    0  \n",
       "2                Adequate                    0  \n",
       "3                Adequate                    0  \n",
       "4                Adequate                    0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" print(f'Type Label: {type_label_index}')\n",
    "print(f'Effectiveness Label: {effectiveness_label_index}') \"\"\"\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ff82020-5374-43f9-ad2f-53f92bd9a6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' dataset[\"essay_full\"] = dataset[\\'essay_id\\']\\n\\nall_train_files_path = f\\'{dataset_path}/train\\'\\nall_train_files = []\\n\\nfor root, _, files in os.walk(all_train_files_path):\\n    for file in files:\\n        all_train_files.append({\\n            \"file\": file.split(\\'.\\')[0],         # remove extension (.txt)\\n            \"path\": os.path.join(root, file)\\n            })\\n '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" dataset[\"essay_full\"] = dataset['essay_id']\n",
    "\n",
    "all_train_files_path = f'{dataset_path}/train'\n",
    "all_train_files = []\n",
    "\n",
    "for root, _, files in os.walk(all_train_files_path):\n",
    "    for file in files:\n",
    "        all_train_files.append({\n",
    "            \"file\": file.split('.')[0],         # remove extension (.txt)\n",
    "            \"path\": os.path.join(root, file)\n",
    "            })\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34d244c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for file in all_train_files:\\n    file_path = file[\"path\"]\\n    file_name = file[\"file\"]\\n    file_content = \\'\\'\\n\\n    with open(file_path, \\'r\\') as f:\\n        file_content = f.read()\\n\\n    dataset[\"essay_full\"].replace(file_name, file_content, inplace=True)\\n '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" for file in all_train_files:\n",
    "    file_path = file[\"path\"]\n",
    "    file_name = file[\"file\"]\n",
    "    file_content = ''\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    dataset[\"essay_full\"].replace(file_name, file_content, inplace=True)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd75f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" dataset['essay_full'] = dataset['essay_full'].map(lambda x: clean_text(x))\\ndataset['discourse_text'] = dataset['discourse_text'].map(lambda x: clean_text(x)) \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" dataset['essay_full'] = dataset['essay_full'].map(lambda x: clean_text(x))\n",
    "dataset['discourse_text'] = dataset['discourse_text'].map(lambda x: clean_text(x)) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc8e635",
   "metadata": {},
   "source": [
    "In glove che abbiamo sono presenti anche i caratteri come `n't`, `'s` e tutti quelli che abbiamo rimosso. Guardare se e' meglio averli oppure no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f27490fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' embeddings_index = dict()\\n\\nwith open(f\\'{dataset_path}/glove/glove.6B.50d.txt\\', \\'r\\') as f:\\n  for line in f: \\n    values = line.split()\\n    word = values[0]\\n    coefs = np.array(values[1:], dtype = \"float32\")\\n    embeddings_index[word] = coefs\\n '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" embeddings_index = dict()\n",
    "\n",
    "with open(f'{dataset_path}/glove/glove.6B.50d.txt', 'r') as f:\n",
    "  for line in f: \n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.array(values[1:], dtype = \"float32\")\n",
    "    embeddings_index[word] = coefs\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "738466a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' x = dataset[[\"essay_full\", \"discourse_text\", \"type_label\"]]\\ny = dataset[[\"effectiveness_label\"]] '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" x = dataset[[\"essay_full\", \"discourse_text\", \"type_label\"]]\n",
    "y = dataset[[\"effectiveness_label\"]] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1953612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' essay_full = x[\"essay_full\"]\\ndiscourse_text = x[\"discourse_text\"]\\ntype_label = x[\"type_label\"] '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" essay_full = x[\"essay_full\"]\n",
    "discourse_text = x[\"discourse_text\"]\n",
    "type_label = x[\"type_label\"] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae6ebe53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" tokenizer = Tokenizer(filters='')\\ntokenizer.fit_on_texts(essay_full) \""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(essay_full) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9ff4b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' vocab_size = len(tokenizer.word_index) + 1 # adding 1 because of reserverd 0 index '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" vocab_size = len(tokenizer.word_index) + 1 # adding 1 because of reserverd 0 index \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b554bc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' embeddings_matrix = np.zeros((vocab_size, 50))   # 50 = embedding dimension\\nhits = 0\\nmisses = 0\\n\\nfor word, index in tokenizer.word_index.items():\\n  # print(word, index)\\n  if index > vocab_size-1:\\n    break\\n  else:\\n    embedding_vector = embeddings_index.get(word)\\n    if embedding_vector is not None:\\n        embeddings_matrix[index] = embedding_vector\\n        hits += 1\\n    else:\\n      print(word, index)\\n      misses += 1\\nprint(\"Converted %d words (%d misses)\" % (hits, misses)) '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" embeddings_matrix = np.zeros((vocab_size, 50))   # 50 = embedding dimension\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "  # print(word, index)\n",
    "  if index > vocab_size-1:\n",
    "    break\n",
    "  else:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[index] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "      print(word, index)\n",
    "      misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af345d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=MAX_LEN):\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for text in texts:\n",
    "        token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length',\n",
    "                         add_special_tokens=True)\n",
    "        input_ids.append(token['input_ids'])\n",
    "        token_type_ids.append(token['token_type_ids'])\n",
    "        attention_mask.append(token['attention_mask'])\n",
    "    \n",
    "    return np.array(input_ids), np.array(token_type_ids), np.array(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b7892da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./prova/tokenizer_config.json',\n",
       " './prova/special_tokens_map.json',\n",
       " './prova/vocab.txt',\n",
       " './prova/added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('./dataset/bert2/')\n",
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('./prova/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "627dcb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep = tokenizer.sep_token\n",
    "sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b79c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['inputs'] = dataset.discourse_type + sep +dataset.discourse_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e8eefe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>effectiveness_label</th>\n",
       "      <th>inputs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>0</td>\n",
       "      <td>Lead[SEP]Hi, i'm Isaac, i'm going to be writin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9704a709b505</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>On my perspective, I think that the face is a ...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>0</td>\n",
       "      <td>Position[SEP]On my perspective, I think that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c22adee811b6</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>I think that the face is a natural landform be...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>0</td>\n",
       "      <td>Claim[SEP]I think that the face is a natural l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a10d361e54e4</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>If life was on Mars, we would know by now. The...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>0</td>\n",
       "      <td>Evidence[SEP]If life was on Mars, we would kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db3e453ec4e2</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>People thought that the face was formed by ali...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>0</td>\n",
       "      <td>Counterclaim[SEP]People thought that the face ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id      essay_id  \\\n",
       "0  0013cc385424  007ACE74B050   \n",
       "1  9704a709b505  007ACE74B050   \n",
       "2  c22adee811b6  007ACE74B050   \n",
       "3  a10d361e54e4  007ACE74B050   \n",
       "4  db3e453ec4e2  007ACE74B050   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Hi, i'm Isaac, i'm going to be writing about h...           Lead   \n",
       "1  On my perspective, I think that the face is a ...       Position   \n",
       "2  I think that the face is a natural landform be...          Claim   \n",
       "3  If life was on Mars, we would know by now. The...       Evidence   \n",
       "4  People thought that the face was formed by ali...   Counterclaim   \n",
       "\n",
       "  discourse_effectiveness  effectiveness_label  \\\n",
       "0                Adequate                    0   \n",
       "1                Adequate                    0   \n",
       "2                Adequate                    0   \n",
       "3                Adequate                    0   \n",
       "4                Adequate                    0   \n",
       "\n",
       "                                              inputs  \n",
       "0  Lead[SEP]Hi, i'm Isaac, i'm going to be writin...  \n",
       "1  Position[SEP]On my perspective, I think that t...  \n",
       "2  Claim[SEP]I think that the face is a natural l...  \n",
       "3  Evidence[SEP]If life was on Mars, we would kno...  \n",
       "4  Counterclaim[SEP]People thought that the face ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0f8dd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' essay_full = tokenizer.texts_to_sequences(essay_full)\\ndiscourse_text = tokenizer.texts_to_sequences(discourse_text)\\n '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" essay_full = tokenizer.texts_to_sequences(essay_full)\n",
    "discourse_text = tokenizer.texts_to_sequences(discourse_text)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43059c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' maxlen_essay = max(len(elem) for elem in essay_full) '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" maxlen_essay = max(len(elem) for elem in essay_full) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6ff6ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' maxlen_discourse = max(len(elem) for elem in discourse_text) '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" maxlen_discourse = max(len(elem) for elem in discourse_text) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5700b4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' essay_full = pad_sequences(essay_full, padding=\"post\", maxlen=maxlen_essay)\\ndiscourse_text = pad_sequences(discourse_text, padding=\"post\", maxlen=maxlen_discourse) '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" essay_full = pad_sequences(essay_full, padding=\"post\", maxlen=maxlen_essay)\n",
    "discourse_text = pad_sequences(discourse_text, padding=\"post\", maxlen=maxlen_discourse) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c4744ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a = []\\nfor i in range(len(essay_full)):\\n   a.append([essay_full[i], discourse_text[i], type_label[i]])\\n '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" a = []\n",
    "for i in range(len(essay_full)):\n",
    "   a.append([essay_full[i], discourse_text[i], type_label[i]])\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "039e5b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a = np.asarray(a, dtype=\"object\") '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" a = np.asarray(a, dtype=\"object\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d54c344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29412,)\n",
      "(7353,)\n",
      "(29412,)\n",
      "(7353,)\n",
      "<class 'pandas.core.series.Series'>\n",
      "Train\n",
      "\tData: Lead[SEP]Hi, i'm Isaac, i'm going to be writing about how this face on Mars is a natural landform or if there is life on Mars that made it. The story is about how NASA took a picture of Mars and a face was seen on the planet. NASA doesn't know if the landform was created by life on Mars, or if it is just a natural landform. \n",
      "\tLabel: 0\n"
     ]
    }
   ],
   "source": [
    "# suddivide il dataset in una parte di training e in una di validation\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset[\"inputs\"], dataset[\"effectiveness_label\"], test_size=.2, shuffle=True)\n",
    "\n",
    "# TODO da provare senza shuffle \n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(type(x_test))\n",
    "\n",
    "print(\"Train\")\n",
    "print(f'\\tData: {x_train[0]}')\n",
    "print(f'\\tLabel: {y_train[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0f2a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = bert_encode(x_train.astype(str), tokenizer)\n",
    "x_test = bert_encode(x_test.astype(str), tokenizer)\n",
    "\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97f45822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101   140 20737 ...     0     0     0]\n",
      " [  101   140 20737 ...     0     0     0]\n",
      " [  101   140 20737 ...     0     0     0]\n",
      " ...\n",
      " [  101   140 20737 ...     0     0     0]\n",
      " [  101 23064 16659 ...     0     0     0]\n",
      " [  101   140 20737 ...     0     0     0]]\n",
      "\n",
      "\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "\n",
      "[[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "\n",
      "\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print('\\n')\n",
    "print(x_train[1])\n",
    "print('\\n')\n",
    "print(x_train[2])\n",
    "print('\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "621e49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "# Configuration\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed0888c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_test, y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475cffb2",
   "metadata": {},
   "source": [
    "## Modelo Bello Bello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a64dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bce939a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./dataset/bert2/ were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ./dataset/bert2/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_1 (TFBertModel)   TFBaseModelOutputWit 108310272   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli (None, 768)          0           tf_bert_model_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 768)          0           tf.__operators__.getitem_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            2307        dropout_75[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 108,312,579\n",
      "Trainable params: 108,312,579\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(bert_model, max_len=MAX_LEN):    \n",
    "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    token_type_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"token_type_ids\")\n",
    "    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    sequence_output = bert_model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "\n",
    "    clf_output = Dropout(.1)(clf_output)\n",
    "    #dense_1 = Dense(124)(clf_output)\n",
    "    #dense_2 = Dense(32)(dense_1)\n",
    "    out = Dense(3, activation='softmax')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=out)\n",
    "    model.compile(optimizer = \"adam\", loss =\"sparse_categorical_crossentropy\", metrics = [\"accuracy\"] )\n",
    "    \n",
    "    return model\n",
    "\n",
    "asd = (TFBertModel.from_pretrained('./dataset/bert2/'))\n",
    "model = build_model(asd)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0401f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 16:15:43.629970: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/100 [>.............................] - ETA: 8:42 - loss: 6.6122 - accuracy: 0.3125"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/work/BRUN0-Net/training/Brun0-Net.ipynb Cell 41'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6f7465626f6f6b227d/home/jovyan/work/BRUN0-Net/training/Brun0-Net.ipynb#ch0000045vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m\"\"\" x_train = np.asarray(x_train).astype(np.float32) \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6f7465626f6f6b227d/home/jovyan/work/BRUN0-Net/training/Brun0-Net.ipynb#ch0000045vscode-remote?line=1'>2</a>\u001b[0m \u001b[39my_train = np.asarray(y_train).astype(np.float32) \"\"\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6f7465626f6f6b227d/home/jovyan/work/BRUN0-Net/training/Brun0-Net.ipynb#ch0000045vscode-remote?line=3'>4</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6f7465626f6f6b227d/home/jovyan/work/BRUN0-Net/training/Brun0-Net.ipynb#ch0000045vscode-remote?line=4'>5</a>\u001b[0m     train_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6f7465626f6f6b227d/home/jovyan/work/BRUN0-Net/training/Brun0-Net.ipynb#ch0000045vscode-remote?line=5'>6</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mtest_dataset, \n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6f7465626f6f6b227d/home/jovyan/work/BRUN0-Net/training/Brun0-Net.ipynb#ch0000045vscode-remote?line=6'>7</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6f7465626f6f6b227d/home/jovyan/work/BRUN0-Net/training/Brun0-Net.ipynb#ch0000045vscode-remote?line=7'>8</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6f7465626f6f6b227d/home/jovyan/work/BRUN0-Net/training/Brun0-Net.ipynb#ch0000045vscode-remote?line=8'>9</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6f7465626f6f6b227d/home/jovyan/work/BRUN0-Net/training/Brun0-Net.ipynb#ch0000045vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39m#validation_data=(x_test, y_test), \u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6f7465626f6f6b227d/home/jovyan/work/BRUN0-Net/training/Brun0-Net.ipynb#ch0000045vscode-remote?line=10'>11</a>\u001b[0m     \u001b[39m#batch_size=32\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e6f7465626f6f6b227d/home/jovyan/work/BRUN0-Net/training/Brun0-Net.ipynb#ch0000045vscode-remote?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   3037\u001b[0m   (graph_function,\n\u001b[1;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m     args,\n\u001b[1;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1968\u001b[0m     executing_eagerly)\n\u001b[1;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" x_train = np.asarray(x_train).astype(np.float32) \n",
    "y_train = np.asarray(y_train).astype(np.float32) \"\"\"\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset, \n",
    "    epochs=1, \n",
    "    verbose=True, \n",
    "    steps_per_epoch=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" https://storage.googleapis.com/kaggle-data-sets/1335671/2233309/upload/bert-base-cased.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220615%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220615T154743Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=518334c07352fa9a8e94ff0c790e2c776ae796c9298f47879e655c457bad451165430e377234ea3bf8a4761d6c96913ff909ac36b9a717920c7d586423a2f390915b6514436ec8728d597ba023d7535b6a7aacd3496bc79a9ed1ccdf9773a9ef98fbdddf1000feb14ec1a5694e5cdca7edc9408a5ab18a6283a25ef50aedfbfa86966de897e9a52ed1bf0ce4fe28550f356d08d821831e3c61759814dd3ba0c86dc1c00974d9ff00ba59a9b1ae1d4de5167ff8dc5ba81b3c3dc369fca93c5e94bd1ab8f22e12b070118fc9bb2b4f02a4e4311a17cf6a18a572932f9145b81d1302feb9cd59c6a0594ae3bf275b1edd480705a6a3300fc5d194f1e484f5fd9dc5 \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
